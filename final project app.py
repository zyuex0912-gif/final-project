import streamlit as st
import requests
import json
from openai import OpenAI  # éœ€å®‰è£… openai åº“ï¼ˆè‹¥ç”¨å…¶ä»–AIæ¨¡å‹å¯æ›¿æ¢ï¼‰
import time

# ---------------------- é…ç½®é¡¹ ----------------------
# API é…ç½®
GBIF_API_URL = "https://api.gbif.org/v1/species"
INATURALIST_API_URL = "https://api.inaturalist.org/v1/taxa"

# OpenAI é…ç½®ï¼ˆå¯æ›¿æ¢ä¸º Claudeã€Gemini ç­‰å…¶ä»– AI æ¨¡å‹ï¼‰
st.secrets["OPENAI_API_KEY"] = st.text_input("è¯·è¾“å…¥ä½ çš„ OpenAI API Key", type="password")
client = OpenAI(api_key=st.secrets.get("OPENAI_API_KEY"))

# åŠ¨ç‰©å›­é¦†é•¿è§’è‰²æç¤ºè¯ï¼ˆæ ¸å¿ƒï¼‰
CURATOR_PROMPT = """
ä½ ç°åœ¨æ˜¯å…¨çƒé¡¶çº§åŠ¨ç‰©å›­çš„èµ„æ·±é¦†é•¿ï¼Œæ‹¥æœ‰30å¹´åŠ¨ç‰©ä¿è‚²å’Œç§‘æ™®ç»éªŒï¼Œæ“…é•¿ç”¨ç”ŸåŠ¨ã€é€šä¿—ä¸”ä¸“ä¸šçš„è¯­è¨€å‘å‚è§‚è€…ä»‹ç»åŠ¨ç‰©ã€‚
è¯·åŸºäºä»¥ä¸‹åŠ¨ç‰©æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½å®Œæ•´çš„è§£è¯´ï¼š
1. å¼€å¤´ç”¨äº²åˆ‡çš„é—®å€™å¸å¼•æ³¨æ„åŠ›ï¼ˆå¦‚â€œå„ä½å‚è§‚è€…ï¼Œæ¬¢è¿æ¥åˆ°XXå±•åŒºï¼â€ï¼‰ï¼›
2. æ ¸å¿ƒå†…å®¹åŒ…å«ï¼šåŠ¨ç‰©çš„ä¸­æ–‡å/è‹±æ–‡å/å­¦åã€å¤–å½¢ç‰¹å¾ã€ç”Ÿæ´»ä¹ æ€§ï¼ˆé£Ÿæ€§ã€æ –æ¯åœ°ã€è¡Œä¸ºç‰¹ç‚¹ï¼‰ã€åœ°ç†åˆ†å¸ƒã€ä¿æŠ¤çŠ¶æ€ï¼›
3. åŠ å…¥1-2ä¸ªè¶£å‘³å†·çŸ¥è¯†ï¼ˆå¦‚ç‹¬ç‰¹çš„ç”Ÿå­˜æŠ€èƒ½ã€æ°‘é—´ä¿—ç§°ç”±æ¥ç­‰ï¼‰ï¼›
4. ç»“å°¾åŠ ä¸Šä¿æŠ¤å€¡è®®ï¼Œä¼ é€’ç”Ÿæ€ä¿æŠ¤ç†å¿µï¼›
5. è¯­æ°”å‹å¥½ã€å£è¯­åŒ–ï¼Œé¿å…è¿‡äºå­¦æœ¯åŒ–ï¼Œé€‚åˆå…¨å¹´é¾„æ®µå‚è§‚è€…ã€‚

åŠ¨ç‰©æ•°æ®ï¼š
{animal_data}
"""

# ---------------------- å·¥å…·å‡½æ•° ----------------------
@st.cache_data(ttl=3600)  # ç¼“å­˜1å°æ—¶ï¼Œé¿å…é‡å¤è°ƒç”¨API
def fetch_gbif_data(species_name=None, region=None):
    """ä» GBIF API è·å–ç‰©ç§åŸºç¡€æ•°æ®ï¼ˆåˆ†ç±»ã€åˆ†å¸ƒã€ä¿æŠ¤çŠ¶æ€ï¼‰"""
    params = {}
    if species_name:
        params["name"] = species_name
        params["rank"] = "SPECIES"  # åªæŸ¥è¯¢ç‰©ç§çº§æ•°æ®
    if region:
        params["country"] = region  # å›½å®¶ä»£ç ï¼ˆå¦‚ CN=ä¸­å›½ï¼ŒUS=ç¾å›½ï¼‰
    
    try:
        response = requests.get(GBIF_API_URL, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        if data.get("results"):
            return data["results"][0]  # è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…ç»“æœ
        return None
    except Exception as e:
        st.warning(f"GBIF API è°ƒç”¨å¤±è´¥ï¼š{str(e)}")
        return None

@st.cache_data(ttl=3600)
def fetch_inaturalist_data(species_name=None, max_photos=3):
    """ä» iNaturalist API è·å–ç‰©ç§å›¾ç‰‡ã€æ°‘é—´è§‚æµ‹æ•°æ®ã€ç”Ÿæ´»ä¹ æ€§"""
    params = {
        "q": species_name,
        "rank": "species",
        "per_page": 1,
        "photos": True
    }
    
    try:
        response = requests.get(INATURALIST_API_URL, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        if data.get("results"):
            result = data["results"][0]
            # æå–å…³é”®ä¿¡æ¯
            inat_data = {
                "common_name": result.get("preferred_common_name"),
                "habitat": result.get("habitat"),
                "behavior": result.get("behavior"),
                "photos": [photo["url"] for photo in result.get("photos", [])[:max_photos]],
                "observations_count": result.get("observations_count", 0)
            }
            return inat_data
        return None
    except Exception as e:
        st.warning(f"iNaturalist API è°ƒç”¨å¤±è´¥ï¼š{str(e)}")
        return None

def merge_animal_data(gbif_data, inat_data):
    """åˆå¹¶ GBIF å’Œ iNaturalist æ•°æ®ï¼Œç”Ÿæˆç»Ÿä¸€çš„åŠ¨ç‰©ä¿¡æ¯å­—å…¸"""
    if not gbif_data:
        return None
    
    merged = {
        "scientific_name": gbif_data.get("scientificName", "æœªçŸ¥å­¦å"),
        "chinese_name": gbif_data.get("vernacularName", inat_data.get("common_name", "æœªçŸ¥ä¸­æ–‡å")),
        "english_name": gbif_data.get("englishName", "æœªçŸ¥è‹±æ–‡å"),
        "classification": {
            "kingdom": gbif_data.get("kingdom"),
            "phylum": gbif_data.get("phylum"),
            "class": gbif_data.get("class"),
            "order": gbif_data.get("order"),
            "family": gbif_data.get("family"),
            "genus": gbif_data.get("genus")
        },
        "distribution": gbif_data.get("distribution", {}).get("countries", ["æœªçŸ¥åˆ†å¸ƒ"]),
        "conservation_status": gbif_data.get("status", "æœªçŸ¥ä¿æŠ¤çŠ¶æ€"),
        "habitat": inat_data.get("habitat", "æœªçŸ¥æ –æ¯åœ°"),
        "behavior": inat_data.get("behavior", "æœªçŸ¥è¡Œä¸ºä¹ æ€§"),
        "photos": inat_data.get("photos", []),
        "observations_count": inat_data.get("observations_count", 0)
    }
    return merged

def generate_curator_explanation(animal_data):
    """è°ƒç”¨ AI ç”ŸæˆåŠ¨ç‰©å›­é¦†é•¿é£æ ¼çš„è§£è¯´"""
    if not st.secrets.get("OPENAI_API_KEY"):
        st.error("è¯·å…ˆè¾“å…¥ä½ çš„ OpenAI API Keyï¼")
        return None
    
    prompt = CURATOR_PROMPT.format(animal_data=json.dumps(animal_data, ensure_ascii=False))
    
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,  # æ§åˆ¶è¯­è¨€ç”ŸåŠ¨åº¦
            max_tokens=1000
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        st.warning(f"AI è§£è¯´ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
        return None

# ---------------------- Streamlit ç•Œé¢è®¾è®¡ ----------------------
st.set_page_config(page_title="AI åŠ¨ç‰©å›­é¦†é•¿", page_icon="ğŸ˜", layout="wide")
st.title("ğŸ… AI åŠ¨ç‰©å›­é¦†é•¿")
st.subheader("â€”â€” åŸºäºçœŸå®ç”Ÿç‰©æ•°æ®çš„æ™ºèƒ½åŠ¨ç‰©è§£è¯´")

# ä¾§è¾¹æ ï¼šç­›é€‰æ¡ä»¶
with st.sidebar:
    st.header("ğŸ” ç­›é€‰æ¡ä»¶")
    region = st.selectbox(
        "é€‰æ‹©åœ°åŒº",
        options=["", "CN", "US", "JP", "AU", "DE"],
        format_func=lambda x: {"": "å…¨çƒ", "CN": "ä¸­å›½", "US": "ç¾å›½", "JP": "æ—¥æœ¬", "AU": "æ¾³å¤§åˆ©äºš", "DE": "å¾·å›½"}[x]
    )
    search_name = st.text_input("è¾“å…¥åŠ¨ç‰©åç§°ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰")
    search_btn = st.button("æœç´¢åŠ¨ç‰©")

# ä¸»ç•Œé¢ï¼šå±•ç¤ºåŒºåŸŸ
col1, col2 = st.columns([1, 2])  # å·¦ä¾§å›¾ç‰‡åŒºï¼Œå³ä¾§è§£è¯´åŒº

if search_btn and search_name:
    with st.spinner("æ­£åœ¨è·å–åŠ¨ç‰©æ•°æ®å¹¶ç”Ÿæˆè§£è¯´..."):
        # 1. è°ƒç”¨åŒ API è·å–æ•°æ®
        gbif_data = fetch_gbif_data(species_name=search_name, region=region)
        inat_data = fetch_inaturalist_data(species_name=search_name)
        
        # 2. åˆå¹¶æ•°æ®
        animal_data = merge_animal_data(gbif_data, inat_data)
        if not animal_data:
            st.error("æœªæŸ¥è¯¢åˆ°è¯¥åŠ¨ç‰©æ•°æ®ï¼Œè¯·æ›´æ¢åç§°æˆ–åœ°åŒºé‡è¯•ï¼")
            st.stop()
        
        # 3. ç”Ÿæˆ AI è§£è¯´
        explanation = generate_curator_explanation(animal_data)
        
        # 4. å±•ç¤ºç»“æœ
        with col1:
            st.subheader(f"ğŸ¾ {animal_data['chinese_name']}")
            st.caption(f"å­¦åï¼š{animal_data['scientific_name']}")
            st.caption(f"ä¿æŠ¤çŠ¶æ€ï¼š{animal_data['conservation_status']}")
            
            # å±•ç¤ºåŠ¨ç‰©å›¾ç‰‡
            if animal_data["photos"]:
                for photo in animal_data["photos"]:
                    st.image(photo, use_column_width=True, caption="å®æ‹å›¾ç‰‡ï¼ˆæ¥è‡ª iNaturalistï¼‰")
            else:
                st.image("https://via.placeholder.com/400x300?text=æš‚æ— å›¾ç‰‡", use_column_width=True)
            
            # åŸºç¡€ä¿¡æ¯å¡ç‰‡
            st.divider()
            st.info(f"ğŸŒ åˆ†å¸ƒåœ°åŒºï¼š{', '.join(animal_data['distribution'])}")
            st.info(f"ğŸ•ï¸ æ –æ¯åœ°ï¼š{animal_data['habitat']}")
            st.info(f"ğŸ‘€ å…¨çƒè§‚æµ‹è®°å½•ï¼š{animal_data['observations_count']:,} æ¡")
        
        with col2:
            st.subheader("ğŸ¤ é¦†é•¿è§£è¯´")
            if explanation:
                st.markdown(f"<div style='font-size:18px; line-height:1.8;'>{explanation}</div>", unsafe_allow_html=True)
            else:
                st.warning("è§£è¯´ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key æˆ–ç½‘ç»œè¿æ¥ï¼")

else:
    # åˆå§‹é¡µé¢å±•ç¤ºç¤ºä¾‹åŠ¨ç‰©
    st.divider()
    st.subheader("ğŸŒŸ çƒ­é—¨åŠ¨ç‰©æ¨è")
    
    # é¢„åŠ è½½å‡ ä¸ªå¸¸è§åŠ¨ç‰©ç¤ºä¾‹
    example_species = ["å¤§ç†ŠçŒ«", "éæ´²è±¡", "ä¸œåŒ—è™", "è“é²¸"]
    cols = st.columns(len(example_species))
    
    for i, species in enumerate(example_species):
        with cols[i]:
            st.image(f"https://via.placeholder.com/200x150?text={species}", use_column_width=True)
            st.button(f"æŸ¥çœ‹ {species} è§£è¯´", key=species, on_click=lambda s=species: st.session_state.update({"selected_example": s}))
    
    # å¤„ç†ç¤ºä¾‹åŠ¨ç‰©ç‚¹å‡»
    if "selected_example" in st.session_state:
        selected_species = st.session_state["selected_example"]
        with st.spinner(f"æ­£åœ¨åŠ è½½ {selected_species} æ•°æ®..."):
            gbif_data = fetch_gbif_data(species_name=selected_species)
            inat_data = fetch_inaturalist_data(species_name=selected_species)
            animal_data = merge_animal_data(gbif_data, inat_data)
            explanation = generate_curator_explanation(animal_data)
            
            # å±•ç¤ºç¤ºä¾‹ç»“æœ
            with col1:
                st.subheader(f"ğŸ¾ {animal_data['chinese_name']}")
                st.caption(f"å­¦åï¼š{animal_data['scientific_name']}")
                if animal_data["photos"]:
                    st.image(animal_data["photos"][0], use_column_width=True, caption="å®æ‹å›¾ç‰‡ï¼ˆæ¥è‡ª iNaturalistï¼‰")
                else:
                    st.image("https://via.placeholder.com/400x300?text=æš‚æ— å›¾ç‰‡", use_column_width=True)
                
                st.divider()
                st.info(f"ğŸŒ åˆ†å¸ƒåœ°åŒºï¼š{', '.join(animal_data['distribution'])}")
                st.info(f"ğŸ•ï¸ æ –æ¯åœ°ï¼š{animal_data['habitat']}")
            
            with col2:
                st.subheader("ğŸ¤ é¦†é•¿è§£è¯´")
                st.markdown(f"<div style='font-size:18px; line-height:1.8;'>{explanation}</div>", unsafe_allow_html=True)

# ---------------------- åº•éƒ¨ä¿¡æ¯ ----------------------
st.divider()
st.caption("ğŸ“Š æ•°æ®æ¥æºï¼šGBIF APIï¼ˆå…¨çƒç”Ÿç‰©å¤šæ ·æ€§ä¿¡æ¯ç½‘ç»œï¼‰ã€iNaturalist APIï¼ˆå…¬æ°‘ç§‘å­¦é¡¹ç›®ï¼‰")
st.caption("ğŸ¤– AI æ¨¡å‹ï¼šOpenAI GPT-3.5 Turboï¼ˆå¯æ›¿æ¢ä¸ºå…¶ä»–å¤§è¯­è¨€æ¨¡å‹ï¼‰")

